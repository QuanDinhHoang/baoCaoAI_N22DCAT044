# baoCaoAI_N22DCAT044

My Project : Face classification using Convolutional Neural Networks (CNN) on the LFW dataset

# 1. LFW Dataset (Labeled Faces in the Wild)

The Labeled Faces in the Wild (LFW) dataset is a facial image database developed to explore the problem of unconstrained face recognition.

- **Purpose:** Released for research to advance face verification, not for commercial algorithm evaluation.
- **Goal:** Evaluate the accuracy of face recognition systems under uncontrolled real-world conditions (in-the-wild).

## ğŸ”‘ Key characteristics of the LFW dataset:

- ğŸ“¸ **Number of images:** 13,233 facial images  
- ğŸ‘¥ **Number of individuals:** 5,749 different people  
- ğŸ” **Images per individual:** Uneven; ~1,680 people have >1 image  
- ğŸŒ **Image sources:** Collected from the Internet (mainly news websites)  
- ğŸ·ï¸ **Labeling:** Each image is labeled with the personâ€™s name  
- ğŸ–¼ï¸ **Image size:** 250Ã—250 pixels (DeepFunneled version aligned & normalized)



# 2. Convolutional Neural Network (CNN)

A **Convolutional Neural Network (CNN)**, also known as *ConvNet*, is a specialized type of deep learning algorithm primarily designed for object recognition tasks such as image classification. CNNs are widely used in real-world applications such as self-driving cars, security camera systems, and many other domains.

## ğŸ” Why CNNs Are Important

CNNs have become essential in modern AI due to several key advantages:

- ğŸš€ **Superior performance** over traditional machine learning algorithms (e.g., SVM, decision trees) by automatically extracting large-scale features â€” no manual feature engineering needed.
- ğŸ§  **Translation invariance**: Convolutional layers allow CNNs to detect and extract patterns regardless of changes in position, orientation, scale, or displacement.
- ğŸ—ï¸ **Pre-trained architectures** like `VGG-16`, `ResNet50`, `Inceptionv3`, and `EfficientNet` offer strong performance and can be fine-tuned for new tasks with limited data.
- ğŸ”„ **Versatility**: CNNs go beyond image classification â€” theyâ€™re also used in:
  - ğŸ“ˆ Time series analysis  
  - ğŸ—£ï¸ Speech recognition  
  - ğŸ“ Natural Language Processing (NLP)

## ğŸ§© Main Components of a CNN

A CNN typically consists of four key components:

1. **Convolutional Layers** â€“ for automatic feature extraction.  
2. **Rectified Linear Units (ReLU)** â€“ for introducing non-linearity.  
3. **Pooling Layers** â€“ commonly MaxPooling, used to downsample feature maps.  
4. **Fully Connected Layers** â€“ for final classification based on extracted features.


# 3. About My Project

## ğŸ“ Dataset Preparation â€“ LFW (Labeled Faces in the Wild)

- âœ… **Step 1**: The program iterates through all subdirectories in the LFW dataset, counting the number of images per individual.
- âœ… **Step 2**: Only individuals with **â‰¥ 30 images** are selected to ensure a sufficiently large dataset and to avoid training bias.
- âœ… **Step 3**: Using the `face_recognition` library, the program detects and extracts face regions from the selected images.
- âœ… **Step 4**: Each face image is resized to a standard size of **100Ã—100 pixels**.
- âœ… **Step 5**: Corresponding labels are encoded into numeric format using `LabelEncoder`.
- âœ… **Step 6**: The dataset is split into:
  - **Training set**: 80%  
  - **Validation set**: 10%  
  - **Test set**: 10%  
  using **stratified sampling** to preserve class distribution across all subsets.

---

## ğŸ§  Model Architecture & Training

- ğŸ§© **Model Structure**:
  - Built using **4 blocks** of:
    - `Conv2D`
    - `BatchNormalization`
    - `MaxPooling2D`
  - Followed by **2 Dense layers** with a `Dropout` of **0.6** to prevent overfitting.

- âš™ï¸ **Training Configuration**:
  - **Loss function**: `categorical_crossentropy`
  - **Optimizer**: `Adam`
  - **Epochs**: 60
  - **Batch size**: 64
  - **Validation**: Model performance is monitored on the validation set during training.


## ğŸš€ 4. Installation & Execution

### ğŸ”¹ Clone the Project

> âš ï¸ The program is configured to load the LFW dataset using an absolute path, so please clone the project into the **D drive**:

```bash
git clone https://github.com/QuanDinhHoang/baoCaoAI_N22DCAT044.git
```
### ğŸ”¹Set Up the Environment
Download and install Anaconda from the official website:
ğŸ‘‰ https://www.anaconda.com/download/success

Create a new virtual environment (recommended: Python 3.10):
```bash
conda create -n lfw_env python=3.10
conda activate lfw_env
```

### ğŸ“¦ Install Required Libraries
Install all required Python libraries using pip:
```bash
pip install -r requirements.txt
```
If requirements.txt is not available, manually install the libraries listed in the documentation.

ğŸ“ Set the Dataset Path
In the train.py file, update the DATASET_PATH to match your local dataset directory:
```bash
DATASET_PATH = r"D:\baoCaoAI_N22DCAT044\archive\lfw-deepfunneled\lfw-deepfunneled"
```
âš ï¸ Ensure the dataset is extracted to this path or update it based on your dataset's actual location.

## â–¶ï¸ Run the Training Script
Once everything is set up, activate your environment and execute the training file:
```bash
python D:\baoCaoAI_N22DCAT044\train.py
```










